{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9cfd851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14a7d5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that takes actual labels and predicted labels and calculates accuracy\n",
    "def accuracy(actual_labels, pred_labels):\n",
    "    n = len(actual_labels)\n",
    "    correct_preds = 0\n",
    "    for i in range(n):\n",
    "        if actual_labels[i] == pred_labels[i]:\n",
    "            correct_preds += 1\n",
    "            \n",
    "    return correct_preds/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18b7bd1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy([1,0,1,1,1,1,0,0,1,0], [1,1,1,1,0,1,0,0,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80d3c609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of True Positives :  8\n",
      "Number of True Negatives :  6\n"
     ]
    }
   ],
   "source": [
    "evaluate([1,1,1,1,1,1,1,1,0,0,0,0,0,0,1,0,1,0,1], [1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,1,0,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a1d1549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that takes actual labels and predicted labels and prints number of TP,TN,FP,FN\n",
    "def evaluate(actual_labels, pred_labels):\n",
    "    true_positives = 0\n",
    "    true_negatives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    \n",
    "    n = len(actual_labels)\n",
    "    for i in range(n):\n",
    "        if actual_labels[i] == pred_labels[i]:\n",
    "            if actual_labels[i] == 1:\n",
    "                true_positives += 1\n",
    "            else:\n",
    "                true_negatives += 1\n",
    "                \n",
    "        else:\n",
    "            if pred_labels[i] == 0:\n",
    "                false_negatives += 1\n",
    "            else:\n",
    "                false_positives += 1\n",
    "                \n",
    "    print('Number of True Positives  : ', true_positives)\n",
    "    print('Number of True Negatives  : ', true_negatives)\n",
    "    print('Number of False Positives : ', false_positives)\n",
    "    print('Number of False Negatives : ', false_negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74008cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of True Positives  :  8\n",
      "Number of True Negatives  :  6\n",
      "Number of False Positives :  2\n",
      "Number of False Negatives :  3\n"
     ]
    }
   ],
   "source": [
    "evaluate([1,1,1,1,1,1,1,1,0,0,0,0,0,0,1,0,1,0,1], [1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,1,0,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64d1b9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that takes actual labels and predicted labels and returns a dictionary having of TP,TN,FP,FN\n",
    "def evaluate(actual_labels, pred_labels):\n",
    "    true_positives = true_negatives = false_positives = false_negatives = 0\n",
    "    \n",
    "    n = len(actual_labels)\n",
    "    for i in range(n):\n",
    "        if actual_labels[i] == pred_labels[i]:\n",
    "            if actual_labels[i] == 1:\n",
    "                true_positives += 1\n",
    "            else:\n",
    "                true_negatives += 1\n",
    "                \n",
    "        else:\n",
    "            if pred_labels[i] == 0:\n",
    "                false_negatives += 1\n",
    "            else:\n",
    "                false_positives += 1\n",
    "                \n",
    "    result = {'TP' : true_positives, 'TN' : true_negatives, 'FP' : false_positives, 'FN' : false_negatives}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "390d69a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TP': 8, 'TN': 7, 'FP': 2, 'FN': 3}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate([1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,1,0,1,0,1], [1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,1,0,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8b9fdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function which takes actual labels and predicted labels and prints Accuracy, Precision, Recall and f1 score\n",
    "def report(actual_labels, pred_labels):\n",
    "    data = evaluate(actual_labels, pred_labels)\n",
    "    tp, tn, fn, fp = data['TP'], data['TN'], data['FN'], data['FP']\n",
    "    accuracy = (tn+tp)/(tn+tp+fn+fp)\n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    f1_score = 2 * precision * recall/(precision + recall)\n",
    "    \n",
    "    print('Accuracy  : ', accuracy)\n",
    "    print('Precision : ', precision)\n",
    "    print('Recall    : ', recall)\n",
    "    print('F1_Score  : ', f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1246203f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy  :  0.75\n",
      "Precision :  0.8\n",
      "Recall    :  0.7272727272727273\n",
      "F1_Score  :  0.761904761904762\n"
     ]
    }
   ],
   "source": [
    "report([1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,1,0,1,0,1], [1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,1,0,1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d186c16a",
   "metadata": {},
   "source": [
    "# ==============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec7a1f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function which takes the number of cases beloging to two classes and returns the entropy\n",
    "def calc_entropy2(c1, c2):\n",
    "    if c1 + c2 == 0:\n",
    "        return 'No cases present'\n",
    "    elif c1 == 0 or c2 == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        total = c1 + c2\n",
    "        p1 = c1 / total\n",
    "        p2 = c2 / total\n",
    "\n",
    "        entropy = -(p1*np.log2(p1) + p2*np.log2(p2))\n",
    "        return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb310568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No cases present'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_entropy2(0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a81cdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function which takes the number of cases belonging to two classes and returns the gini index\n",
    "def calc_gini2(c1, c2):\n",
    "    if c1 + c2 == 0:\n",
    "        return 'No cases present'\n",
    "    else:\n",
    "        total = c1 + c2\n",
    "        p1 = c1 / total\n",
    "        p2 = c2 / total\n",
    "\n",
    "        gini = 1 - (p1**2 + p2**2)\n",
    "        return gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a949422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No cases present'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_gini2(0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "14c2f116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_entropy(*classes):\n",
    "    total = sum(classes)\n",
    "    if total == 0:\n",
    "        print('No cases present')\n",
    "    else:\n",
    "        probs = [i/total for i in classes]\n",
    "        entropy = -sum([p*np.log2(p) for p in probs if p != 0])\n",
    "        return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "42702832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9779810607523773"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_entropy(20,12,12,47,69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "18b90cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7435821353206307"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_entropy(20,12,12,47,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51899b1",
   "metadata": {},
   "source": [
    "# ===================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67f1b18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that takes actual_values and predicted values and returns the mean absolute error\n",
    "def calc_mae(actual_labels, pred_labels):\n",
    "    if len(actual_labels) != len(pred_labels):\n",
    "        print('The length of actual labels and predicted labels must be same')\n",
    "    else:\n",
    "        n = len(actual_labels)\n",
    "        error = 0\n",
    "        for i in range(n):\n",
    "            error = error + abs(actual_labels[i] - pred_labels[i])\n",
    "        return error/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ec52d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2400.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_mae([60000, 80000, 70000, 90000, 85000], [63000, 78000, 72000, 86000, 86000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7734b5a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2400.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error([60000, 80000, 70000, 90000, 85000], [63000, 78000, 72000, 86000, 86000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e16a54f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that takes actual values and predicted values and returns the mean squared error\n",
    "def calc_mse(actual_labels, pred_labels):\n",
    "    if len(actual_labels) != len(pred_labels):\n",
    "        print('The length of actual labels and predicted labels must be same')\n",
    "    else:\n",
    "        n = len(actual_labels)\n",
    "        error = 0\n",
    "        for i in range(n):\n",
    "            error = error + (actual_labels[i] - pred_labels[i]) ** 2\n",
    "        return error/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8525e53e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6800000.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_mse([60000, 80000, 70000, 90000, 85000], [63000, 78000, 72000, 86000, 86000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bac474f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6800000.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error([60000, 80000, 70000, 90000, 85000], [63000, 78000, 72000, 86000, 86000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1f0a48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that takes actual values and predicted values and returns the r2_score\n",
    "def calc_r2_score(actual_labels, pred_labels):\n",
    "    if len(actual_labels) != len(pred_labels):\n",
    "        print('The length of actual labels and predicted labels must be same')\n",
    "    else:\n",
    "        n = len(actual_labels)\n",
    "        mean = sum(actual_labels) / n\n",
    "        rss = 0\n",
    "        tss = 0\n",
    "        for i in range(n):\n",
    "            rss = rss + (actual_labels[i] - pred_labels[i])**2\n",
    "            tss = tss + (actual_labels[i] - mean)**2\n",
    "        \n",
    "            \n",
    "        r2_score = 1 - (rss/tss)\n",
    "        return r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd5df166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9413793103448276"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_r2_score([60000, 80000, 70000, 90000, 85000], [63000, 78000, 72000, 86000, 86000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3603fb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9413793103448276"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score([60000, 80000, 70000, 90000, 85000], [63000, 78000, 72000, 86000, 86000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "47bdf9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that takes a list of values as input and returns the z scores\n",
    "def calc_z_score(nums):\n",
    "    n = len(nums)\n",
    "    mean = sum(nums) / n\n",
    "    diff = 0\n",
    "    for i in nums:\n",
    "        diff = diff + (i - mean)**2\n",
    "    std = (diff/n) ** 0.5\n",
    "    \n",
    "    z_scores = [(x - mean)/std for x in nums]\n",
    "    return z_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e96298db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_z_score([10,20,30,40,50,60,70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fb765b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
