{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "- Tokenization\n",
        "- Sequencing\n",
        "- Padding\n",
        "- Stemming\n",
        "- Lemmatization"
      ],
      "metadata": {
        "id": "wFwGZw-LmwL9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Tokenization"
      ],
      "metadata": {
        "id": "44o8T2Ivmw4a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BwVNS1bFjed_"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = ['We love machine learning']\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentence)"
      ],
      "metadata": {
        "id": "lN9ZDpVljjeU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oyvcbad_kA2r",
        "outputId": "63405635-9f91-471a-af33-09bacdcfd5a9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'we': 1, 'love': 2, 'machine': 3, 'learning': 4}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# repeating words\n",
        "sentence = ['we love machine learning and deep learning']\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentence)\n",
        "tokenizer.word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsMkl1VpkECi",
        "outputId": "0775917d-a3c7-4d73-bce6-b40f52e70790"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'learning': 1, 'we': 2, 'love': 3, 'machine': 4, 'and': 5, 'deep': 6}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization in not case sensitive\n",
        "# Tokenisation removes special characters\n",
        "sentence = ['@ We love machine LEARNING .....!!! and deep learning']\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentence)\n",
        "tokenizer.word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEuA-Cr3kwAR",
        "outputId": "d017d190-6327-4261-cee4-618e6fb87262"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'learning': 1, 'we': 2, 'love': 3, 'machine': 4, 'and': 5, 'deep': 6}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = ['We are learning natural language processing',\n",
        "             'We have learned computer vision',\n",
        "             'We are learning from a good trainer']\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "tokenizer.word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnX-nyDLlw0-",
        "outputId": "9796b9af-b62d-469a-e8cf-96e0b0d80d80"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'we': 1,\n",
              " 'are': 2,\n",
              " 'learning': 3,\n",
              " 'natural': 4,\n",
              " 'language': 5,\n",
              " 'processing': 6,\n",
              " 'have': 7,\n",
              " 'learned': 8,\n",
              " 'computer': 9,\n",
              " 'vision': 10,\n",
              " 'from': 11,\n",
              " 'a': 12,\n",
              " 'good': 13,\n",
              " 'trainer': 14}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Sequencing"
      ],
      "metadata": {
        "id": "4dZqN5yZmqyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = ['We are learning text preprocessing',\n",
        "             'Tokenization refers to representing each word as a token',\n",
        "             'Sequencing refers to representing text as a sequence of tokens',\n",
        "             'Padding refers to adding zeros to sequences to make them all of same length']\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "tokenizer.word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_6ZOUeimcb1",
        "outputId": "d6ce8e5c-048b-423b-af8d-13707dcb73ff"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'to': 1,\n",
              " 'refers': 2,\n",
              " 'text': 3,\n",
              " 'representing': 4,\n",
              " 'as': 5,\n",
              " 'a': 6,\n",
              " 'of': 7,\n",
              " 'we': 8,\n",
              " 'are': 9,\n",
              " 'learning': 10,\n",
              " 'preprocessing': 11,\n",
              " 'tokenization': 12,\n",
              " 'each': 13,\n",
              " 'word': 14,\n",
              " 'token': 15,\n",
              " 'sequencing': 16,\n",
              " 'sequence': 17,\n",
              " 'tokens': 18,\n",
              " 'padding': 19,\n",
              " 'adding': 20,\n",
              " 'zeros': 21,\n",
              " 'sequences': 22,\n",
              " 'make': 23,\n",
              " 'them': 24,\n",
              " 'all': 25,\n",
              " 'same': 26,\n",
              " 'length': 27}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9UtpTGxnbzH",
        "outputId": "1d8417f6-1fef-40e6-fedb-cfebf6ab7206"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[8, 9, 10, 3, 11],\n",
              " [12, 2, 1, 4, 13, 14, 5, 6, 15],\n",
              " [16, 2, 1, 4, 3, 5, 6, 17, 7, 18],\n",
              " [19, 2, 1, 20, 21, 1, 22, 1, 23, 24, 25, 7, 26, 27]]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.texts_to_sequences(['Text preprocessing involves tokenization, sequencing and padding'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbOkBP8Enr3C",
        "outputId": "88e42aaf-a7e5-4636-e249-a20aa06ea159"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[3, 11, 12, 16, 19]]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.texts_to_sequences(['Text preprocessing does not involve tokenization, sequencing and padding'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plbaa_n_o4UL",
        "outputId": "4e343074-c96b-48ce-dd28-75dd4d40ab98"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[3, 11, 12, 16, 19]]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Out of vocabulary\n",
        "sentences = ['We are learning text preprocessing',\n",
        "             'Tokenization refers to representing each word as a token',\n",
        "             'Sequencing refers to representing text as a sequence of tokens',\n",
        "             'Padding refers to adding zeros to sequences to make them all of same length']\n",
        "\n",
        "tokenizer = Tokenizer(oov_token = '#OOV')\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "tokenizer.word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_vhcLvMpWHA",
        "outputId": "83a02b0a-b71b-4358-b009-969ba7efee57"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'#OOV': 1,\n",
              " 'to': 2,\n",
              " 'refers': 3,\n",
              " 'text': 4,\n",
              " 'representing': 5,\n",
              " 'as': 6,\n",
              " 'a': 7,\n",
              " 'of': 8,\n",
              " 'we': 9,\n",
              " 'are': 10,\n",
              " 'learning': 11,\n",
              " 'preprocessing': 12,\n",
              " 'tokenization': 13,\n",
              " 'each': 14,\n",
              " 'word': 15,\n",
              " 'token': 16,\n",
              " 'sequencing': 17,\n",
              " 'sequence': 18,\n",
              " 'tokens': 19,\n",
              " 'padding': 20,\n",
              " 'adding': 21,\n",
              " 'zeros': 22,\n",
              " 'sequences': 23,\n",
              " 'make': 24,\n",
              " 'them': 25,\n",
              " 'all': 26,\n",
              " 'same': 27,\n",
              " 'length': 28}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.texts_to_sequences(['Text preprocessing involves tokenization, sequencing and padding'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlDtE4hwp2id",
        "outputId": "58e7723b-07da-4702-ef97-d27910834be2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[4, 12, 1, 13, 17, 1, 20]]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.texts_to_sequences(['Text preprocessing does not involve tokenization, sequencing and padding'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CEWNX5YqF33",
        "outputId": "6329ff1c-c5e6-4098-edd1-fef2c784bb93"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[4, 12, 1, 1, 1, 13, 17, 1, 20]]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Padding"
      ],
      "metadata": {
        "id": "uy0v64kQqrrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = ['We love machine learning',\n",
        "             'We are learning tokenization',\n",
        "             'we are learning sequencing',\n",
        "             'we are learning the technique of padding',\n",
        "             'Machine learning and deep learning are fun']\n",
        "\n",
        "tokenizer = Tokenizer(oov_token = '#OOV')\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eon8xMwkqZBa",
        "outputId": "3cc3e7ef-a3e8-46f9-938e-a8a7eb0b5dd3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[3, 6, 5, 2],\n",
              " [3, 4, 2, 7],\n",
              " [3, 4, 2, 8],\n",
              " [3, 4, 2, 9, 10, 11, 12],\n",
              " [5, 2, 13, 14, 2, 4, 15]]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "padded_sequences = pad_sequences(sequences)\n",
        "padded_sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEYbgT71r-mK",
        "outputId": "349e4aab-3521-4548-d951-4ab6e58c44c1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  0,  0,  3,  6,  5,  2],\n",
              "       [ 0,  0,  0,  3,  4,  2,  7],\n",
              "       [ 0,  0,  0,  3,  4,  2,  8],\n",
              "       [ 3,  4,  2,  9, 10, 11, 12],\n",
              "       [ 5,  2, 13, 14,  2,  4, 15]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded_sequences = pad_sequences(sequences, padding = 'pre')\n",
        "padded_sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NM8UPAGQsnbC",
        "outputId": "3cd62ab7-0cde-4fd6-d51a-4c812ccaea34"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  0,  0,  3,  6,  5,  2],\n",
              "       [ 0,  0,  0,  3,  4,  2,  7],\n",
              "       [ 0,  0,  0,  3,  4,  2,  8],\n",
              "       [ 3,  4,  2,  9, 10, 11, 12],\n",
              "       [ 5,  2, 13, 14,  2,  4, 15]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded_sequences = pad_sequences(sequences, padding = 'post')\n",
        "padded_sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VE-xEx9ctHbN",
        "outputId": "8b69d6bf-a310-44d7-8df1-dbfd43e02390"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 3,  6,  5,  2,  0,  0,  0],\n",
              "       [ 3,  4,  2,  7,  0,  0,  0],\n",
              "       [ 3,  4,  2,  8,  0,  0,  0],\n",
              "       [ 3,  4,  2,  9, 10, 11, 12],\n",
              "       [ 5,  2, 13, 14,  2,  4, 15]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = ['We love machine learning',\n",
        "             'We are learning tokenization',\n",
        "             'we are learning sequencing',\n",
        "             'we are learning the technique of padding',\n",
        "             'Machine learning and deep learning are fun',\n",
        "             'The purpose behind text preprocessing in to give a numerical representation to text data']\n",
        "\n",
        "tokenizer = Tokenizer(oov_token = '#OOV')\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kaB6i-qtVzQ",
        "outputId": "98da71c2-09dd-4e18-b1f4-f43e1f9a803c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[3, 9, 5, 2],\n",
              " [3, 4, 2, 10],\n",
              " [3, 4, 2, 11],\n",
              " [3, 4, 2, 6, 12, 13, 14],\n",
              " [5, 2, 15, 16, 2, 4, 17],\n",
              " [6, 18, 19, 7, 20, 21, 8, 22, 23, 24, 25, 8, 7, 26]]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded_sequences = pad_sequences(sequences, padding = 'pre')\n",
        "padded_sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgtNQ9A-uJM_",
        "outputId": "11b600b9-d887-44ae-a3c6-c34e9bcbfb89"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  3,  9,  5,  2],\n",
              "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  3,  4,  2, 10],\n",
              "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  3,  4,  2, 11],\n",
              "       [ 0,  0,  0,  0,  0,  0,  0,  3,  4,  2,  6, 12, 13, 14],\n",
              "       [ 0,  0,  0,  0,  0,  0,  0,  5,  2, 15, 16,  2,  4, 17],\n",
              "       [ 6, 18, 19,  7, 20, 21,  8, 22, 23, 24, 25,  8,  7, 26]],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded_sequences = pad_sequences(sequences, padding = 'pre', maxlen = 8)\n",
        "padded_sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nF_ySg7uUQ3",
        "outputId": "0e58df53-d3af-4908-9b61-78b643920ace"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  0,  0,  0,  3,  9,  5,  2],\n",
              "       [ 0,  0,  0,  0,  3,  4,  2, 10],\n",
              "       [ 0,  0,  0,  0,  3,  4,  2, 11],\n",
              "       [ 0,  3,  4,  2,  6, 12, 13, 14],\n",
              "       [ 0,  5,  2, 15, 16,  2,  4, 17],\n",
              "       [ 8, 22, 23, 24, 25,  8,  7, 26]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded_sequences = pad_sequences(sequences, padding = 'pre', maxlen = 8, truncating = 'pre')\n",
        "padded_sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PFPrPuAvEtT",
        "outputId": "c8a72b21-8eac-4764-c3d2-a1923ab0d002"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  0,  0,  0,  3,  9,  5,  2],\n",
              "       [ 0,  0,  0,  0,  3,  4,  2, 10],\n",
              "       [ 0,  0,  0,  0,  3,  4,  2, 11],\n",
              "       [ 0,  3,  4,  2,  6, 12, 13, 14],\n",
              "       [ 0,  5,  2, 15, 16,  2,  4, 17],\n",
              "       [ 8, 22, 23, 24, 25,  8,  7, 26]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded_sequences = pad_sequences(sequences, padding = 'pre', maxlen = 8, truncating = 'post')\n",
        "padded_sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ppcqXIivYQz",
        "outputId": "4bf155b9-d95f-41e8-c3ab-4aa12f3a5de1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  0,  0,  0,  3,  9,  5,  2],\n",
              "       [ 0,  0,  0,  0,  3,  4,  2, 10],\n",
              "       [ 0,  0,  0,  0,  3,  4,  2, 11],\n",
              "       [ 0,  3,  4,  2,  6, 12, 13, 14],\n",
              "       [ 0,  5,  2, 15, 16,  2,  4, 17],\n",
              "       [ 6, 18, 19,  7, 20, 21,  8, 22]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "oMY42fIHv1hX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete Process\n",
        "sentences = ['We love machine learning',\n",
        "             'We are learning tokenization',\n",
        "             'we are learning sequencing',\n",
        "             'we are learning the technique of padding',\n",
        "             'Machine learning and deep learning are fun',\n",
        "             'The purpose behind text preprocessing in to give a numerical representation to text data']\n",
        "\n",
        "tokenizer = Tokenizer(oov_token = '#OOV')                   # Create an object of tokenizer class\n",
        "tokenizer.fit_on_texts(sentences)                           # Tokenization\n",
        "sequences = tokenizer.texts_to_sequences(sentences)         # Sequencing\n",
        "padded_sequences = pad_sequences(sequences, maxlen = 10)    # Padding\n",
        "padded_sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUmB3BuAvgEc",
        "outputId": "db769ce6-3126-44e8-8dd8-3608ca6a8467"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  0,  0,  0,  0,  0,  3,  9,  5,  2],\n",
              "       [ 0,  0,  0,  0,  0,  0,  3,  4,  2, 10],\n",
              "       [ 0,  0,  0,  0,  0,  0,  3,  4,  2, 11],\n",
              "       [ 0,  0,  0,  3,  4,  2,  6, 12, 13, 14],\n",
              "       [ 0,  0,  0,  5,  2, 15, 16,  2,  4, 17],\n",
              "       [20, 21,  8, 22, 23, 24, 25,  8,  7, 26]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ============================================================================"
      ],
      "metadata": {
        "id": "k9I513XixZ2k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Stemming"
      ],
      "metadata": {
        "id": "8BEWiiBIyqaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "stemmer.stem('breaking')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "XcY29vLTwvpi",
        "outputId": "11e8c929-1b83-4d51-9f65-b922b0834b77"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'break'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer.stem('Writing')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "uTKUnUatzAMb",
        "outputId": "3a5a769d-83e3-470f-865d-63f553a298dd"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'write'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(stemmer.stem('breaks'))\n",
        "print(stemmer.stem('breaking'))\n",
        "print(stemmer.stem('broke'))\n",
        "print(stemmer.stem('broken'))\n",
        "print(stemmer.stem('changes'))\n",
        "print(stemmer.stem('changed'))\n",
        "print(stemmer.stem('changing'))\n",
        "print(stemmer.stem('writes'))\n",
        "print(stemmer.stem('writing'))\n",
        "print(stemmer.stem('running'))\n",
        "print(stemmer.stem('ran'))\n",
        "print(stemmer.stem('run'))\n",
        "print(stemmer.stem('trouble'))\n",
        "print(stemmer.stem('troubled'))\n",
        "print(stemmer.stem('troubling'))\n",
        "print(stemmer.stem('cats'))\n",
        "print(stemmer.stem('knives'))\n",
        "print(stemmer.stem('leaves'))\n",
        "print(stemmer.stem('jumping'))\n",
        "print(stemmer.stem('jumped'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goS14_qlzF1c",
        "outputId": "f05b9fa5-91d4-4c31-a5aa-dbef963a906f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "break\n",
            "break\n",
            "broke\n",
            "broken\n",
            "chang\n",
            "chang\n",
            "chang\n",
            "write\n",
            "write\n",
            "run\n",
            "ran\n",
            "run\n",
            "troubl\n",
            "troubl\n",
            "troubl\n",
            "cat\n",
            "knive\n",
            "leav\n",
            "jump\n",
            "jump\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Lemmatization"
      ],
      "metadata": {
        "id": "L8KuDwVU0MC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBQPDdNnzyOH",
        "outputId": "c0cf7884-c615-4b7c-8163-9ec992d0ed96"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatizer.lemmatize('breaks')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "m1SQnduy0YU-",
        "outputId": "4a3bb386-6b04-40b5-a18b-d1c27d4235aa"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'break'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize('breaks', pos = 'v')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "u9WzWfSx0oBH",
        "outputId": "373f99ea-5ac1-49a3-f80b-29082095d5be"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'break'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(lemmatizer.lemmatize('breaks', pos = 'v'))\n",
        "print(lemmatizer.lemmatize('breaking', pos = 'v'))\n",
        "print(lemmatizer.lemmatize('broke', pos = 'v'))\n",
        "print(lemmatizer.lemmatize('broken', pos = 'v'))\n",
        "print(lemmatizer.lemmatize('changes', pos = 'v'))\n",
        "print(lemmatizer.lemmatize('changed', pos = 'v'))\n",
        "print(lemmatizer.lemmatize('changing', pos = 'v'))\n",
        "print(lemmatizer.lemmatize('writes', pos = 'v'))\n",
        "print(lemmatizer.lemmatize('writing', pos = 'v'))\n",
        "print(lemmatizer.lemmatize('running', pos = 'v'))\n",
        "print(lemmatizer.lemmatize('ran', pos = 'v'))\n",
        "print(lemmatizer.lemmatize('run', pos = 'v'))\n",
        "print(lemmatizer.lemmatize('trouble', pos = 'v'))\n",
        "print(lemmatizer.lemmatize('troubled', pos = 'v'))\n",
        "print(lemmatizer.lemmatize('troubling', pos = 'v'))\n",
        "print(lemmatizer.lemmatize('cats', pos = 'v'))\n",
        "print(lemmatizer.lemmatize('knives'))\n",
        "print(lemmatizer.lemmatize('leaves'))\n",
        "print(lemmatizer.lemmatize('jumping', pos = 'v'))\n",
        "print(lemmatizer.lemmatize('jumped', pos = 'v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4UPN_fE0u_j",
        "outputId": "329abe45-54ca-48f5-bf03-d6cd7232b8a6"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "break\n",
            "break\n",
            "break\n",
            "break\n",
            "change\n",
            "change\n",
            "change\n",
            "write\n",
            "write\n",
            "run\n",
            "run\n",
            "run\n",
            "trouble\n",
            "trouble\n",
            "trouble\n",
            "cat\n",
            "knife\n",
            "leaf\n",
            "jump\n",
            "jump\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uEdaTxG01IPh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}